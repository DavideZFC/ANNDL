{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Baroni_Cerisola_Maran.ipynb","provenance":[],"collapsed_sections":["odr5SQcVecy5","CgrzOagDZ8qm","rLvYBtu0Z8qn","TUoFbJDD3QVh","BNYJx_Xm4CBQ","MXFdXiknLuCk","kjE6saBHL5OF","ZoUrPrm7uLQk","EaDqS3RDrys8","eaB1F6uisJu3","rzGAPwKEr26l"],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Zv7qo1hEGp_2"},"source":["# **Gruppo \"Deep Learning Warlords\"**\n","\n","Jean Paul Guglielmo **Baroni**\\\n","Maurizio **Cerisola**\\\n","Davide **Maran**"]},{"cell_type":"markdown","metadata":{"id":"rH7Xuc8GOgVJ"},"source":["**INDEX:**\n","1. Utilities\\\n","     1.1 Dataset creation\\\n","     1.2 Dataset Dimension Analysis\\\n","     1.3 GPU check\\\n","     1.4 Misclassified images check\n","\n","2. Image preprocessing\n","\n","3. Ternary models\\\n","     3.1 No transfer learning\\\n","     3.2 InceptionResNetV2\\\n","     3.3 DenseNet201 (K_3 Model)\n","\n","4. Binary models\\\n","     4.1 K_M model\\\n","     4.2 K_F model\n","     \n","5. Bayesian ensemble\n"]},{"cell_type":"markdown","metadata":{"id":"b2QP7Iu8Z-d_"},"source":["\n","**1.1**\\\n","NOTE: the MaskDataset folder should be placed in the same folder as this notebook.\\\n","In order to load the database, we divided the training set in the 3 **subfolders \"0\", \"1\", \"2\"**.\n","\n","**1.2**\\\n","In order to set the CNN input size, itâ€™s important to study the dataset images sizes since changing images proportions provides a loss of information.\\\n","At first we used squared inputs, but after looking at the dataset composition, we switched to 612x408 and it proved to be a more effective choice.\n","\n","**1.3**\\\n","In our instance, we opted for **Colab GPUs**: we implemented a check to understand whether the accellerators were properly set and to possibly set the Memory Growth.\n","\n","**1.4**\\\n","The \"getMisclassified\" function allows us to visually check which images were misclassified by **showing them and building a confusion matrix**.\n","\n","**2**\\\n","To load the data and make them suitable for feeding the neural network, we use the ImageDataGenerator. We also agumented the data using a small shift, rotation and zoom range because we think that these transformation preserve the faces. Moreover, we used the horizontal flip since faces are nearly symmetric under this transformation, while they aren't under vertical flip.\n","\n","**3.1**\\\n","The first model we developed is a standard CNN composed of a conv2D-ReLU-MaxPool2D layer sequences, but the huge number of trainable parameters (TP) leads to overfitting and unsatisfactory results.\n","After many trials, we decided not to further investigate architectures without transfer learning.\n","\n","**3.2**\\\n","The first backbone which provided noteworthy results has been InceptionResNetV2 with weights taken from imagenet.\\\n","At first we used a simple NN without fine tuning: results are slightly better than the ones of 3.1, but a lot can be done in order to improve.\\\n","The first HP we started playing with, is the number of InceptionResNetV2 freezed layers, this is particularly important, since it will influence the number of TP and so it will higly affect overfitting problems.\\\n","Another important choice is the architecture of the NN top. A very shallow top does not allow the CNN to achieve good classification results while a very deep top have overfitting problems even though droput layers might help to mitigate them. As regards the activation functions, we tried many of them and sigmoid proved to be the best performing one.\n","\n","**3.3**\\\n","DenseNet201 proved to be a better performing backbone than InceptionResNetV2.\\\n","Just like in the previous case, we spent a lot of time with trial-and-error HP tuning, we also got many tips from the DenseNet201 reference paper (https://arxiv.org/abs/1608.06993). <br>\n","This was the best ternary model and we called it K_3.\n","\n","**4**\\\n","To improve our predictions, we created two models that have 2, instead of 3 outputs:\n","\n","*   K_M detects if there is a mask in the picture, so it's trained only on classes \"0\" vs \"2\"\n","*   K_F detects unmasked faces, so it's trained only on classes \"1\" vs \"2\"\n","\n","An intense trial and error procedure led to the used HP.\n","\n","**4.1**\\\n","The K_M model is based on DenseNet201: for binary classifiers we opted for few dense units and epochs, to avoid overfitting.\n","\n","**4.2**\\\n","The K_F model is instead based on InceptionResnetV2.\n","\n","**5**\\\n","The predictions of K_3, K_M, K_F were merged. Based on the interpretation of the outputs as posterior probabilities, we apply a Bayes enseble and choose the class c which maximizes the quantinity\n","\t$$P(c|K_3)*P(c|K_M)*P(c|K_F)$$\n","In order to compute P('0'|K_M) we assume that if an image is said to contain a mask, then it is equally likely for it to be in class '1' or '2'. The same is done for the face detector, with the classes '0','2' instead. Thus, we take\n","\n","\n","*   P('0'|K_M='no mask')=1\n","*   P('1'|K_M='mask')=P('2'|K_MD='mask')=1/2\n","*   P('1'|K_F='no face')=1\n","*   P('0'|K_F='no face')=P('2'|K_FD='no face')=1/2\n","\n","This is just an approximation but has given good results, improving our final test accuracy up to 95%."]},{"cell_type":"markdown","metadata":{"id":"odr5SQcVecy5"},"source":["# **0. Global Settings**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6EAqARKapzX","executionInfo":{"status":"ok","timestamp":1606056389898,"user_tz":-60,"elapsed":59843,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"ee90b977-df87-4ec2-912d-8c2845b3c58d"},"source":["# Loads Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OMvngwzFbD_v"},"source":["import os \n","# Sets working directory\n","#### IMPORTANT : EDIT THE FOLLOWING\n","os.chdir(\"/content/drive/MyDrive/AN2DL/Final\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0AVjYQtZ8qm"},"source":["Required Packages and Global Parameters:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnQGbAgpZ8qm","executionInfo":{"status":"ok","timestamp":1606075018875,"user_tz":-60,"elapsed":4340,"user":{"displayName":"Davide","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhVXmPMa0Xbhhpt6WwiA_RAksjdVHkudwpSApCX=s64","userId":"01235651926330825704"}},"outputId":"02a00202-3c12-4d38-debe-a3f41e42762c"},"source":["#!pip install progressbar\n","!pip install tensorflow\n","\n","class_names = [\"0\",\"1\",\"2\"]\n","img_h, img_w= (408, 612) \n","SEED = 123\n","data_dir = \"MaskDataset/training/\"\n","validation_split = 0.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n","Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u-EIZSw1Z8ql"},"source":["# **1. Utilities**"]},{"cell_type":"markdown","metadata":{"id":"CgrzOagDZ8qm"},"source":["## **1.1 Creation of the dataset**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcxQfybCZ8qm","executionInfo":{"status":"ok","timestamp":1606056594954,"user_tz":-60,"elapsed":62671,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"b0c73592-de50-4525-d888-a90405f17429"},"source":["# Rearranges everything into MaskDataset2\n","from shutil import move\n","import os\n","import json\n","import progressbar\n","\n","\n","# Uploads training json. class_map is the dict mapping images names to their class\n","f=open('MaskDataset/train_gt.json','r')\n","class_map=json.loads(f.read())\n","\n","# Creates the subfolders\n","for cn in class_names:\n","    folder_path = \"MaskDataset/training/\"+cn\n","    if not os.path.exists(folder_path):\n","        os.mkdir(folder_path)\n","\n","# Progress bar\n","bar = progressbar.ProgressBar(maxval=len(class_map), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n","bar.start()\n","progress = 0\n","\n","# Moves the files to the proper class folder\n","for key in class_map:\n","    start_path = \"MaskDataset/training/\" + key\n","    end_path = \"MaskDataset/training/\" + str(class_map[key]) + \"/\" + key\n","    if not os.path.exists(end_path) and os.path.exists(start_path): # no overwriting\n","        move(start_path, end_path)\n","    progress += 1\n","    bar.update(progress)\n","bar.finish()\n","\n","print(\"Number of items in MaskDataset/training: \"+str(len(os.listdir(\"MaskDataset/training\"))))\n","print(\"Number of items in the subfolders:\")\n","for cn in class_names:\n","    print(\"- MaskDataset2/training/\"+cn+\": \"+str(len(os.listdir(\"MaskDataset/training/\"+cn))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[========================================================================] 100%\n"],"name":"stderr"},{"output_type":"stream","text":["Number of items in MaskDataset/training: 3\n","Number of items in the subfolders:\n","- MaskDataset2/training/0: 1900\n","- MaskDataset2/training/1: 1897\n","- MaskDataset2/training/2: 1817\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hnlOBIv_Z8qn"},"source":["## **1.2 Images Dimension Analysis**"]},{"cell_type":"code","metadata":{"id":"_l07lGIZ0uhi"},"source":["# Computes training set images dimensions\n","import os\n","import pandas as pd\n","import progressbar\n","from PIL import Image\n","\n","for cn in class_names:\n","  #Set directory \n","  directory = \"MaskDataset/training/\"+cn\n","\n","  #Number of files in the directory\n","  num_files = len([f for f in os.listdir(directory)if os.path.isfile(os.path.join(directory, f))])\n","\n","  #Initialize dataframe\n","  ImgSizes_test = pd.DataFrame(columns=['#'])\n","  #initialize Progress bar\n","  bar = progressbar.ProgressBar(maxval=num_files, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n","  bar.start()\n","  progress = 0\n","  #For each image\n","  for filename in os.listdir(directory):\n","      #Open it and take sizes\n","      im = Image.open(directory + \"/\" + filename)\n","      width, height = im.size\n","      #Update dataframe\n","      index = str(width) + \"x\" + str(height);\n","      if (index in ImgSizes_test.index):\n","        ImgSizes_test.loc[index] += 1;\n","      else:\n","        ImgSizes_test.loc[index] = [1];\n","      #Update progress bar\n","      progress += 1\n","      bar.update(progress)\n","  bar.finish()\n","\n","  #Sort dataframe\n","  ImgSizes_test = ImgSizes_test.sort_values(by=['#'], ascending=False)\n","  #Display output \n","  print(\"Class\",cn)\n","  print(ImgSizes_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-ZiGf6T067Z"},"source":["# Computes test set images dimensions\n","import os\n","import pandas as pd\n","import progressbar\n","from PIL import Image\n","\n","#Set directory \n","directory = \"MaskDataset/test\"\n","\n","#Number of files in the directory\n","num_files = len([f for f in os.listdir(directory)if os.path.isfile(os.path.join(directory, f))])\n","\n","#Initialize dataframe\n","ImgSizes_test = pd.DataFrame(columns=['#'])\n","#initialize Progress bar\n","bar = progressbar.ProgressBar(maxval=num_files, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n","bar.start()\n","progress = 0\n","#For each image\n","for filename in os.listdir(directory):\n","    #Open it and take sizes\n","    im = Image.open(directory + \"/\" + filename)\n","    width, height = im.size\n","    #Update dataframe\n","    index = str(width) + \"x\" + str(height);\n","    if (index in ImgSizes_test.index):\n","      ImgSizes_test.loc[index] += 1;\n","    else:\n","      ImgSizes_test.loc[index] = [1];\n","    #Update progress bar\n","    progress += 1\n","    bar.update(progress)\n","bar.finish()\n","\n","#Sort dataframe\n","ImgSizes_test = ImgSizes_test.sort_values(by=['#'], ascending=False)\n","#Display output \n","ImgSizes_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLvYBtu0Z8qn"},"source":["## **1.3 Check on the GPUs**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mopsZ9FKZ8qn","executionInfo":{"status":"ok","timestamp":1606061954741,"user_tz":-60,"elapsed":589,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"199e6608-8f7c-4e11-877f-8d3667480f4a"},"source":["# GPU Check\n","import os\n","import tensorflow as tf\n","from tensorboard.plugins.hparams import api as hp\n","import numpy as np\n","import datetime\n","\n","# Set GPU memory growth \n","# Allows to only as much GPU memory as needed\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","  try:\n","    for gpu in gpus:\n","      tf.config.experimental.set_memory_growth(gpu, True)\n","    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","  except RuntimeError as e:\n","    print(e)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 Physical GPUs, 1 Logical GPUs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YgAd4ZzdZ8qn"},"source":["## **1.4 Checks for misclassified images**"]},{"cell_type":"markdown","metadata":{"id":"2TG4EyufZ8qn"},"source":[""]},{"cell_type":"code","metadata":{"id":"h0Q5SVRuZ8qn"},"source":["# Find wrongly classified images \n","from PIL import Image\n","import os\n","import json\n","from tensorflow.keras.models import load_model\n","import progressbar\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","\n","#Recall:\n","# 0: no one with facemask\n","# 1: all with with facemask\n","# 2: someone with and someone without mask\n","class_names = [\"0\",\"1\",\"2\"]\n","\n","def getMisclassified(model, graphics = True):\n","  # Unpacks the dict into 3 lists:\n","  image_names = [] # name of the file\n","  image_class = [] # name of the correct class (according to json)\n","  image_prediction = [] # name of the predicted class (according to the selected model)\n","\n","  for cn in class_names:\n","    folder_path = \"MaskDataset/training/\"+cn\n","    imageslist = os.listdir(folder_path)\n","    bar = progressbar.ProgressBar(maxval=len(imageslist), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n","    bar.start()\n","    progress = 0\n","    for file in imageslist:\n","      image_names.append(file)\n","      image_class.append(cn)\n","      #Get model prediction\n","      img = Image.open(folder_path+\"/\"+file).convert('RGB')\n","      img=img.resize((img_w,img_h))\n","      img_array = np.array(img)\n","      img_array = np.expand_dims(img_array, 0)\n","      img_array = np.true_divide(img_array,255)\n","      v=model.predict(img_array)\n","      image_prediction.append(str(np.argmax(v)))\n","      progress += 1\n","      bar.update(progress)\n","    bar.finish()\n","\n","  # Builds Confusion Matrix\n","  confusion = confusion_matrix(image_class, image_prediction, class_names)\n","\n","  # Displays wrongly classified images\n","  if graphics:\n","    for inm, icl, ipr in zip(image_names, image_class, image_prediction):\n","      if icl != ipr:\n","        img = mpimg.imread(\"MaskDataset/training/\"+icl+\"/\"+inm)\n","        imgplot = plt.imshow(img)\n","        plt.show()\n","        print(inm, \"true class:\", icl, \"predicted:\", ipr)\n","          \n","  return confusion\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kmz147s1bJnb"},"source":["lm = load_model(\"Models/IncRes_e15_sz612x408_fr777_easyTop_btc64_v3\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MN0Kb1lxbKHC"},"source":["getMisclassified(lm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkdgU74ci-nj"},"source":["#**2. Image Preprocessing**\n"]},{"cell_type":"code","metadata":{"id":"sPRWH5GWq41M"},"source":["import tensorflow as tf\n","import numpy as np\n","from keras.utils import to_categorical\n","from keras import Sequential\n","from keras.layers import Dense\n","from keras import Model\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdbfN8Klq8j2"},"source":["\n","def dataLoader(classes = [\"0\",\"1\",\"2\"], bs = 16, SEED=SEED):\n","  # Create training ImageDataGenerator object\n","  train_data_gen = ImageDataGenerator(rotation_range=10,\n","                                      width_shift_range=img_w//20,#10\n","                                      height_shift_range=img_h//20,#10\n","                                      zoom_range=0.1,\n","                                      horizontal_flip=True,\n","                                      # vertical_flip=True,   #meglio toglierlo secondo me \n","                                      fill_mode='constant',\n","                                      cval=0,\n","                                      rescale=1./255,\n","                                      preprocessing_function=None,\n","                                      data_format='channels_last',\n","                                      validation_split=validation_split)\n","  \n","  # Create validation ImageDataGenerator object\n","  valid_data_gen = ImageDataGenerator(rescale=1./255,validation_split=validation_split) \n","  \n","  \n","  # Create training Generator object\n","  training_dir=data_dir\n","  train_gen = train_data_gen.flow_from_directory(training_dir,\n","                                                batch_size=bs,\n","                                                classes=classes,\n","                                                class_mode='categorical',\n","                                                target_size=(img_h, img_w),\n","                                                shuffle=True, \n","                                                subset='training',\n","                                                seed=SEED) \n","  steps_per_epoch=len(train_gen)\n","\n","  # Create validation Generator object\n","  valid_dir=data_dir\n","  valid_gen = valid_data_gen.flow_from_directory(valid_dir, #same as training directory\n","                                                 batch_size=bs,\n","                                                 classes=classes,\n","                                                 class_mode='categorical',\n","                                                 target_size=(img_h, img_w),\n","                                                 shuffle=True, #even in validation, it doesn't change much. It's better to let it see diverse data at each epoch \n","                                                 subset='validation',\n","                                                 seed=SEED) # set as validation data\n","  validation_steps=len(valid_gen)\n","\n","\n","  # Create training Dataset object\n","  num_classes = len(classes) #both for taining and validation\n","  train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n","                                                output_types=(tf.float32, tf.float32),\n","                                                output_shapes=([None, img_h, img_w, 3], [None, num_classes])).repeat()\n","  # Create validation Dataset object\n","  valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n","                                                output_types=(tf.float32, tf.float32),\n","                                                output_shapes=([None, img_h, img_w, 3], [None, num_classes])).repeat()\n","\n","  return train_dataset, valid_dataset, steps_per_epoch, validation_steps, valid_gen"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJXozEEf3Ldp"},"source":["# **3. Ternary Models**"]},{"cell_type":"markdown","metadata":{"id":"TUoFbJDD3QVh"},"source":["## **3.1 No Transfer Learning**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1EaP7pA38L6","executionInfo":{"status":"ok","timestamp":1606062145357,"user_tz":-60,"elapsed":909,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"3b069ee2-6dc4-4d4e-d0f0-e08c603ee1d0"},"source":["### Calls Loader\n","train_dataset, valid_dataset, steps_per_epoch, validation_steps, valid_gen = dataLoader([\"0\",\"1\",\"2\"], bs = 10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2974 images belonging to 2 classes.\n","Found 743 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUj4QrYRenOq","executionInfo":{"status":"ok","timestamp":1606046539686,"user_tz":-60,"elapsed":4223640,"user":{"displayName":"Maurizio Cerisola","photoUrl":"","userId":"12405136361895816479"}},"outputId":"682b1c94-3f9d-47ee-ffbd-7219da50fd1f"},"source":["# No Transfer Learning\n","saveName = \"NoTransLearnMC_ep15_sz612x408_btc10\"\n","\n","## Architecture ##\n","start_f = 8 #Starting number of filters\n","depth = 4 #Depth of our CNN\n","\n","tmodel = tf.keras.Sequential()\n","\n","# Features extraction\n","for i in range(depth):\n","    if i == 0:\n","        input_shape = [img_h, img_w, 3]\n","    else:\n","        input_shape=[None]\n","    tmodel.add(tf.keras.layers.Conv2D(filters=start_f, \n","                                     kernel_size=(3, 3),\n","                                     strides=(1, 1),\n","                                     padding='same',\n","                                     input_shape=input_shape))\n","    tmodel.add(tf.keras.layers.ReLU())\n","    tmodel.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3)))\n","    start_f *= 2 #each time i double the number of filters \n","    \n","# Classifier\n","tmodel.add(tf.keras.layers.Flatten()) #I vectorize the volume \n","tmodel.add(tf.keras.layers.Dense(units=512, activation='relu')) #I apply a classifier\n","tmodel.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n","\n","## Optimization ##\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n","tmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","tmodel.build(input_shape=(None, img_h, img_w, 3))\n","tmodel.summary()\n","\n","## Callbacks for ES and LR Plateau ##\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7,restore_best_weights=True)\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","\n","## Fitting ##\n","tmodel.fit(x=train_dataset,\n","           epochs=15,  \n","           steps_per_epoch=steps_per_epoch, \n","           validation_data=valid_dataset,\n","           validation_steps=validation_steps,\n","           callbacks=[es_callback,LR_adapter_callback,]) \n","\n","## Save model ##\n","tmodel.save('Models/'+saveName)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_5 (Conv2D)            (None, 408, 612, 8)       224       \n","_________________________________________________________________\n","re_lu_5 (ReLU)               (None, 408, 612, 8)       0         \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 136, 204, 8)       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 136, 204, 16)      1168      \n","_________________________________________________________________\n","re_lu_6 (ReLU)               (None, 136, 204, 16)      0         \n","_________________________________________________________________\n","max_pooling2d_6 (MaxPooling2 (None, 45, 68, 16)        0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 45, 68, 32)        4640      \n","_________________________________________________________________\n","re_lu_7 (ReLU)               (None, 45, 68, 32)        0         \n","_________________________________________________________________\n","max_pooling2d_7 (MaxPooling2 (None, 15, 22, 32)        0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 15, 22, 64)        18496     \n","_________________________________________________________________\n","re_lu_8 (ReLU)               (None, 15, 22, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_8 (MaxPooling2 (None, 5, 7, 64)          0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 2240)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 512)               1147392   \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 3)                 1539      \n","=================================================================\n","Total params: 1,173,459\n","Trainable params: 1,173,459\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/15\n","450/450 [==============================] - 1620s 4s/step - loss: 1.1009 - accuracy: 0.3393 - val_loss: 1.0998 - val_accuracy: 0.3351\n","Epoch 2/15\n","450/450 [==============================] - 260s 577ms/step - loss: 1.0945 - accuracy: 0.3740 - val_loss: 1.0475 - val_accuracy: 0.5169\n","Epoch 3/15\n","450/450 [==============================] - 260s 578ms/step - loss: 0.9490 - accuracy: 0.5336 - val_loss: 1.1121 - val_accuracy: 0.4795\n","Epoch 4/15\n","450/450 [==============================] - 259s 576ms/step - loss: 0.7924 - accuracy: 0.6200 - val_loss: 0.6523 - val_accuracy: 0.6800\n","Epoch 5/15\n","450/450 [==============================] - 257s 572ms/step - loss: 0.6808 - accuracy: 0.6759 - val_loss: 0.7887 - val_accuracy: 0.6444\n","Epoch 6/15\n","450/450 [==============================] - 259s 575ms/step - loss: 0.6230 - accuracy: 0.7110 - val_loss: 0.8100 - val_accuracy: 0.6444\n","Epoch 7/15\n","450/450 [==============================] - ETA: 0s - loss: 0.5789 - accuracy: 0.7235\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","450/450 [==============================] - 259s 575ms/step - loss: 0.5789 - accuracy: 0.7235 - val_loss: 0.8164 - val_accuracy: 0.6631\n","Epoch 8/15\n","450/450 [==============================] - 260s 579ms/step - loss: 0.5209 - accuracy: 0.7531 - val_loss: 0.8849 - val_accuracy: 0.6257\n","Epoch 9/15\n","450/450 [==============================] - 260s 578ms/step - loss: 0.5016 - accuracy: 0.7720 - val_loss: 0.8074 - val_accuracy: 0.6631\n","Epoch 10/15\n","450/450 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.7669\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n","450/450 [==============================] - 259s 575ms/step - loss: 0.4903 - accuracy: 0.7669 - val_loss: 0.8347 - val_accuracy: 0.6551\n","Epoch 11/15\n","450/450 [==============================] - 259s 576ms/step - loss: 0.4787 - accuracy: 0.7809 - val_loss: 0.9094 - val_accuracy: 0.6381\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: /content/drive/My Drive/AN2DL/modelli/NoTransLearnMC_ep15_sz612x408_btc10/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BNYJx_Xm4CBQ"},"source":["## **3.2 InceptionResNetV2**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXy6Qj6N4LA3","executionInfo":{"status":"ok","timestamp":1606062145357,"user_tz":-60,"elapsed":909,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"3b069ee2-6dc4-4d4e-d0f0-e08c603ee1d0"},"source":["### Calls Loader\n","train_dataset, valid_dataset, steps_per_epoch, validation_steps, valid_gen = dataLoader([\"0\",\"1\",\"2\"], bs = 64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2974 images belonging to 2 classes.\n","Found 743 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MXFdXiknLuCk"},"source":["### **3.2.1 Transfer Learning**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eglIWenSn0gn","executionInfo":{"status":"ok","timestamp":1606067208029,"user_tz":-60,"elapsed":7327355,"user":{"displayName":"Maurizio Cerisola","photoUrl":"","userId":"12405136361895816479"}},"outputId":"e1c519bc-cc5c-42f4-d4bd-6555d0b99e06"},"source":["#@title\n","saveName = \"IncRes_e15_sz612x408_noFineTuning_easyTop_btc64\"\n","\n","## Architecture ##\n","tmodel = tf.keras.Sequential()\n","#Bottom\n","arch =  tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3), pooling=\"avg\") \n","for layer in arch.layers:\n","  layer.trainable = False\n","tmodel.add(arch)\n","\n","#Output\n","tmodel.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n","\n","## Optimization ##\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n","tmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","tmodel.build(input_shape=(None, img_h, img_w, 3))\n","tmodel.summary()\n","\n","## Callbacks for ES and LR Plateau ##\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7,restore_best_weights=True)\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","\n","## Fitting ##\n","tmodel.fit(x=train_dataset,\n","           epochs=15,  \n","           steps_per_epoch=steps_per_epoch, \n","           validation_data=valid_dataset,\n","           validation_steps=validation_steps,\n","           callbacks=[es_callback,LR_adapter_callback,]) \n","\n","## Save model ##\n","tmodel.save('/content/drive/My Drive/AN2DL/modelli/'+saveName)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n","219062272/219055592 [==============================] - 15s 0us/step\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","inception_resnet_v2 (Functio (None, 1536)              54336736  \n","_________________________________________________________________\n","dense (Dense)                (None, 3)                 4611      \n","=================================================================\n","Total params: 54,341,347\n","Trainable params: 4,611\n","Non-trainable params: 54,336,736\n","_________________________________________________________________\n","Epoch 1/15\n","71/71 [==============================] - 3161s 45s/step - loss: 0.9281 - accuracy: 0.5806 - val_loss: 0.7868 - val_accuracy: 0.6640\n","Epoch 2/15\n","71/71 [==============================] - 285s 4s/step - loss: 0.7207 - accuracy: 0.6979 - val_loss: 0.6810 - val_accuracy: 0.7005\n","Epoch 3/15\n","71/71 [==============================] - 284s 4s/step - loss: 0.6473 - accuracy: 0.7315 - val_loss: 0.6264 - val_accuracy: 0.7148\n","Epoch 4/15\n","71/71 [==============================] - 283s 4s/step - loss: 0.6055 - accuracy: 0.7451 - val_loss: 0.6039 - val_accuracy: 0.7246\n","Epoch 5/15\n","71/71 [==============================] - 284s 4s/step - loss: 0.5807 - accuracy: 0.7511 - val_loss: 0.5846 - val_accuracy: 0.7299\n","Epoch 6/15\n","71/71 [==============================] - 284s 4s/step - loss: 0.5568 - accuracy: 0.7647 - val_loss: 0.5635 - val_accuracy: 0.7424\n","Epoch 7/15\n","71/71 [==============================] - 284s 4s/step - loss: 0.5408 - accuracy: 0.7705 - val_loss: 0.5478 - val_accuracy: 0.7442\n","Epoch 8/15\n","71/71 [==============================] - 284s 4s/step - loss: 0.5292 - accuracy: 0.7774 - val_loss: 0.5394 - val_accuracy: 0.7558\n","Epoch 9/15\n","71/71 [==============================] - 284s 4s/step - loss: 0.5219 - accuracy: 0.7749 - val_loss: 0.5317 - val_accuracy: 0.7594\n","Epoch 10/15\n","71/71 [==============================] - 284s 4s/step - loss: 0.5077 - accuracy: 0.7838 - val_loss: 0.5205 - val_accuracy: 0.7594\n","Epoch 11/15\n","71/71 [==============================] - 283s 4s/step - loss: 0.5017 - accuracy: 0.7834 - val_loss: 0.5235 - val_accuracy: 0.7620\n","Epoch 12/15\n","71/71 [==============================] - 282s 4s/step - loss: 0.4953 - accuracy: 0.7825 - val_loss: 0.5200 - val_accuracy: 0.7540\n","Epoch 13/15\n","71/71 [==============================] - 282s 4s/step - loss: 0.4876 - accuracy: 0.7858 - val_loss: 0.5037 - val_accuracy: 0.7594\n","Epoch 14/15\n","71/71 [==============================] - 282s 4s/step - loss: 0.4756 - accuracy: 0.7950 - val_loss: 0.4972 - val_accuracy: 0.7692\n","Epoch 15/15\n","71/71 [==============================] - 281s 4s/step - loss: 0.4799 - accuracy: 0.7941 - val_loss: 0.5157 - val_accuracy: 0.7585\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: /content/drive/My Drive/AN2DL/modelli/IncRes_e15_sz612x408_noFineTuning_easyTop_btc64/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kjE6saBHL5OF"},"source":["### **3.2.2 Transfer learning with fine tuning** "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cPmIY2wlsbkw","executionInfo":{"status":"ok","timestamp":1605892648048,"user_tz":-60,"elapsed":3243357,"user":{"displayName":"Maurizio Cerisola","photoUrl":"","userId":"12405136361895816479"}},"outputId":"aec93b91-17cf-48d5-b054-48e29116cbb1"},"source":["#@title\n","saveName = \"IncRes_e15_sz612x408_fr777_easyTop_btc64_v3\"\n","\n","## Architecture ##\n","tmodel = tf.keras.Sequential()\n","#Bottom\n","arch =  tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3), pooling=\"avg\") \n","for layer in arch.layers[:777]:\n","  layer.trainable = False\n","tmodel.add(arch)\n","#Top\n","tmodel.add(tf.keras.layers.Dropout(0.2))\n","tmodel.add(Dense(units=128, activation='sigmoid'))\n","tmodel.add(Dense(units=32, activation='sigmoid'))\n","\n","#Output\n","tmodel.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n","\n","## Optimization ##\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n","tmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","tmodel.build(input_shape=(None, img_h, img_w, 3))\n","tmodel.summary()\n","\n","## Callbacks for ES and LR Plateau ##\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7,restore_best_weights=True)\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","\n","## Fitting ##\n","tmodel.fit(x=train_dataset,\n","           epochs=15,  \n","           steps_per_epoch=steps_per_epoch, \n","           validation_data=valid_dataset,\n","           validation_steps=validation_steps,\n","           callbacks=[es_callback,LR_adapter_callback,]) \n","\n","## Save model ##\n","tmodel.save('Models/'+saveName)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","inception_resnet_v2 (Functio (None, 1536)              54336736  \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 1536)              0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 128)               196736    \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 32)                4128      \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 54,537,699\n","Trainable params: 3,397,379\n","Non-trainable params: 51,140,320\n","_________________________________________________________________\n","Epoch 1/15\n","281/281 [==============================] - 319s 1s/step - loss: 0.6035 - accuracy: 0.7471 - val_loss: 0.4204 - val_accuracy: 0.8253\n","Epoch 2/15\n","281/281 [==============================] - 314s 1s/step - loss: 0.3905 - accuracy: 0.8455 - val_loss: 0.3984 - val_accuracy: 0.8244\n","Epoch 3/15\n","281/281 [==============================] - 310s 1s/step - loss: 0.3406 - accuracy: 0.8651 - val_loss: 0.3080 - val_accuracy: 0.8788\n","Epoch 4/15\n","281/281 [==============================] - 307s 1s/step - loss: 0.2955 - accuracy: 0.8869 - val_loss: 0.3435 - val_accuracy: 0.8592\n","Epoch 5/15\n","281/281 [==============================] - 311s 1s/step - loss: 0.2492 - accuracy: 0.9072 - val_loss: 0.3833 - val_accuracy: 0.8440\n","Epoch 6/15\n","281/281 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9127\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","281/281 [==============================] - 316s 1s/step - loss: 0.2230 - accuracy: 0.9127 - val_loss: 0.3363 - val_accuracy: 0.8672\n","Epoch 7/15\n","281/281 [==============================] - 314s 1s/step - loss: 0.1940 - accuracy: 0.9297 - val_loss: 0.3291 - val_accuracy: 0.8743\n","Epoch 8/15\n","281/281 [==============================] - 313s 1s/step - loss: 0.1756 - accuracy: 0.9352 - val_loss: 0.3352 - val_accuracy: 0.8725\n","Epoch 9/15\n","281/281 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9421\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n","281/281 [==============================] - 317s 1s/step - loss: 0.1632 - accuracy: 0.9421 - val_loss: 0.3245 - val_accuracy: 0.8734\n","Epoch 10/15\n","281/281 [==============================] - 311s 1s/step - loss: 0.1584 - accuracy: 0.9481 - val_loss: 0.3211 - val_accuracy: 0.8708\n","INFO:tensorflow:Assets written to: /content/drive/My Drive/AN2DL/modelli/IncResMC_e15_sz612x408_fr777_easyTop_btc64_v3/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZoUrPrm7uLQk"},"source":["## **3.3 DenseNet 201 (K_3 Model)**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2oTL7D2NuK5","executionInfo":{"status":"ok","timestamp":1606062145357,"user_tz":-60,"elapsed":909,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"3b069ee2-6dc4-4d4e-d0f0-e08c603ee1d0"},"source":["### Calls Loader\n","train_dataset, valid_dataset, steps_per_epoch, validation_steps, valid_gen = dataLoader([\"0\",\"1\",\"2\"], bs = 64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2974 images belonging to 2 classes.\n","Found 743 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wm4v2iR_Tw0","executionInfo":{"status":"ok","timestamp":1605886777544,"user_tz":-60,"elapsed":5369451,"user":{"displayName":"Maurizio Cerisola","photoUrl":"","userId":"12405136361895816479"}},"outputId":"a4a8b346-af3a-4225-dc12-3933b8147227"},"source":["#Best performing trinomial model\n","saveName = \"K_3\"\n","\n","## Architecture ##\n","tmodel = tf.keras.Sequential()\n","#Bottom\n","arch = tf.keras.applications.densenet.DenseNet201(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3), pooling=\"avg\")\n","for layer in arch.layers[:650]:\n","  layer.trainable = False\n","tmodel.add(arch)\n","#Top\n","tmodel.add(tf.keras.layers.Dropout(0.5))\n","tmodel.add(Dense(units=128, activation='sigmoid'))\n","tmodel.add(Dense(units=32, activation='sigmoid'))\n","\n","#Output\n","tmodel.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n","\n","## Optimization ##\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n","tmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","tmodel.build(input_shape=(None, img_h, img_w, 3))\n","tmodel.summary()\n","\n","## Callbacks for ES and LR Plateau ##\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7,restore_best_weights=True)\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","\n","## Fitting ##\n","tmodel.fit(x=train_dataset,\n","           epochs=15,  \n","           steps_per_epoch=steps_per_epoch, \n","           validation_data=valid_dataset,\n","           validation_steps=validation_steps,\n","           callbacks=[es_callback,LR_adapter_callback,]) \n","\n","## Save model ##\n","tmodel.save('Models/'+saveName)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","densenet201 (Functional)     (None, 1920)              18321984  \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 1920)              0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 128)               245888    \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 32)                4128      \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 3)                 99        \n","=================================================================\n","Total params: 18,572,099\n","Trainable params: 2,394,627\n","Non-trainable params: 16,177,472\n","_________________________________________________________________\n","Epoch 1/15\n","281/281 [==============================] - 366s 1s/step - loss: 0.5998 - accuracy: 0.7718 - val_loss: 0.3437 - val_accuracy: 0.8743\n","Epoch 2/15\n","281/281 [==============================] - 357s 1s/step - loss: 0.3146 - accuracy: 0.8923 - val_loss: 0.2736 - val_accuracy: 0.8966\n","Epoch 3/15\n","281/281 [==============================] - 353s 1s/step - loss: 0.2349 - accuracy: 0.9187 - val_loss: 0.3431 - val_accuracy: 0.8672\n","Epoch 4/15\n","281/281 [==============================] - 351s 1s/step - loss: 0.1764 - accuracy: 0.9397 - val_loss: 0.2064 - val_accuracy: 0.9314\n","Epoch 5/15\n","281/281 [==============================] - 349s 1s/step - loss: 0.1629 - accuracy: 0.9441 - val_loss: 0.2110 - val_accuracy: 0.9269\n","Epoch 6/15\n","281/281 [==============================] - 349s 1s/step - loss: 0.1349 - accuracy: 0.9564 - val_loss: 0.2039 - val_accuracy: 0.9340\n","Epoch 7/15\n","281/281 [==============================] - 348s 1s/step - loss: 0.1056 - accuracy: 0.9662 - val_loss: 0.2439 - val_accuracy: 0.9234\n","Epoch 8/15\n","281/281 [==============================] - 353s 1s/step - loss: 0.0894 - accuracy: 0.9722 - val_loss: 0.1968 - val_accuracy: 0.9385\n","Epoch 9/15\n","281/281 [==============================] - 350s 1s/step - loss: 0.0800 - accuracy: 0.9746 - val_loss: 0.2869 - val_accuracy: 0.9135\n","Epoch 10/15\n","281/281 [==============================] - 349s 1s/step - loss: 0.0606 - accuracy: 0.9815 - val_loss: 0.2470 - val_accuracy: 0.9287\n","Epoch 11/15\n","281/281 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9797\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","281/281 [==============================] - 347s 1s/step - loss: 0.0638 - accuracy: 0.9797 - val_loss: 0.2278 - val_accuracy: 0.9332\n","Epoch 12/15\n","281/281 [==============================] - 345s 1s/step - loss: 0.0393 - accuracy: 0.9891 - val_loss: 0.2326 - val_accuracy: 0.9367\n","Epoch 13/15\n","281/281 [==============================] - 346s 1s/step - loss: 0.0314 - accuracy: 0.9920 - val_loss: 0.2190 - val_accuracy: 0.9430\n","Epoch 14/15\n","281/281 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9933\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n","281/281 [==============================] - 349s 1s/step - loss: 0.0267 - accuracy: 0.9933 - val_loss: 0.2454 - val_accuracy: 0.9349\n","Epoch 15/15\n","281/281 [==============================] - 351s 1s/step - loss: 0.0218 - accuracy: 0.9949 - val_loss: 0.2482 - val_accuracy: 0.9349\n","INFO:tensorflow:Assets written to: /content/drive/My Drive/AN2DL/modelli/DenseNetMC_ep15_sz612x408_fr650_newTop_btc64_v2/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EaDqS3RDrys8"},"source":["# **4. Binary Models**"]},{"cell_type":"markdown","metadata":{"id":"eaB1F6uisJu3"},"source":["## **4.1 K_M Model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKaqkRUasgZL","executionInfo":{"status":"ok","timestamp":1606062145357,"user_tz":-60,"elapsed":909,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"3b069ee2-6dc4-4d4e-d0f0-e08c603ee1d0"},"source":["### Calls Loader\n","train_dataset, valid_dataset, steps_per_epoch, validation_steps, valid_gen = dataLoader([\"0\",\"2\"], bs = 16)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2974 images belonging to 2 classes.\n","Found 743 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaTtgThAsey_","executionInfo":{"status":"ok","timestamp":1606062160890,"user_tz":-60,"elapsed":7853,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"e2acb9fd-07df-4b57-fa38-f7c9c602b9c3"},"source":["#@title\n","saveName = \"K_M\"\n","\n","## Architecture ##\n","tmodel = tf.keras.Sequential()\n","#Bottom\n","arch = tf.keras.applications.densenet.DenseNet201(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3))\n","for layer in arch.layers[:600]:\n","  layer.trainable = False\n","tmodel.add(arch)\n","\n","#Top\n","tmodel.add(tf.keras.layers.GlobalAveragePooling2D(data_format=None))\n","tmodel.add(tf.keras.layers.Dropout(0.5))\n","tmodel.add(Dense(units=24,activation='sigmoid', activity_regularizer=tf.keras.regularizers.l2(5e-5)))\n","tmodel.add(tf.keras.layers.Dropout(0.5))\n","tmodel.add(Dense(units=6,activation='sigmoid', activity_regularizer=tf.keras.regularizers.l2(1e-4)))\n","\n","#Output\n","tmodel.add(tf.keras.layers.Dense(units=2, activation='softmax'))\n","\n","## Optimization ##\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n","tmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","tmodel.build(input_shape=(None, img_h, img_w, 3))\n","tmodel.summary()\n","\n","## Callbacks for ES and LR Plateau ##\n","# For just 8 epochs the callbacks are not used\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7,restore_best_weights=True)\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n","74842112/74836368 [==============================] - 0s 0us/step\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","densenet201 (Functional)     (None, 12, 19, 1920)      18321984  \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 1920)              0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 1920)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 24)                46104     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 24)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 6)                 150       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 14        \n","=================================================================\n","Total params: 18,368,252\n","Trainable params: 3,851,708\n","Non-trainable params: 14,516,544\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khqXgAeHs5ve","executionInfo":{"status":"ok","timestamp":1606064110304,"user_tz":-60,"elapsed":1944601,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"5cab8b17-3c71-4f78-926d-e5783b2ad677"},"source":["## Fitting ##\n","tmodel.fit(x=train_dataset,\n","           epochs=8,\n","           steps_per_epoch=steps_per_epoch, \n","           validation_data=valid_dataset,\n","           validation_steps=validation_steps,\n","           callbacks=[es_callback,LR_adapter_callback,]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/8\n","186/186 [==============================] - 251s 1s/step - loss: 0.5238 - accuracy: 0.8924 - val_loss: 0.3936 - val_accuracy: 0.9529\n","Epoch 2/8\n","186/186 [==============================] - 240s 1s/step - loss: 0.3553 - accuracy: 0.9684 - val_loss: 0.2677 - val_accuracy: 0.9744\n","Epoch 3/8\n","186/186 [==============================] - 240s 1s/step - loss: 0.2473 - accuracy: 0.9768 - val_loss: 0.1741 - val_accuracy: 0.9879\n","Epoch 4/8\n","186/186 [==============================] - 241s 1s/step - loss: 0.1704 - accuracy: 0.9855 - val_loss: 0.1335 - val_accuracy: 0.9852\n","Epoch 5/8\n","186/186 [==============================] - 237s 1s/step - loss: 0.1360 - accuracy: 0.9852 - val_loss: 0.1135 - val_accuracy: 0.9812\n","Epoch 6/8\n","186/186 [==============================] - 236s 1s/step - loss: 0.1034 - accuracy: 0.9896 - val_loss: 0.1141 - val_accuracy: 0.9785\n","Epoch 7/8\n","186/186 [==============================] - 238s 1s/step - loss: 0.0872 - accuracy: 0.9879 - val_loss: 0.1064 - val_accuracy: 0.9798\n","Epoch 8/8\n","186/186 [==============================] - 239s 1s/step - loss: 0.0735 - accuracy: 0.9902 - val_loss: 0.0804 - val_accuracy: 0.9865\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f8089d134a8>"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99GZXsqOtFYM","executionInfo":{"status":"ok","timestamp":1606064311540,"user_tz":-60,"elapsed":77310,"user":{"displayName":"Jean Paul Guglielmo Baroni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_9x1uzz7p0m28gepuVh2BqzBXpZikxXKILURI1w=s64","userId":"06491736753157658305"}},"outputId":"8cf1d7d6-7d6d-4f72-aa36-1c6685ae3e76"},"source":["tmodel.save('Models/'+saveName)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: Models/K_M/assets\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rzGAPwKEr26l"},"source":["## **4.2 K_F Model**"]},{"cell_type":"code","metadata":{"id":"CLIn5ENRxV_C"},"source":["### Calls Loader\n","train_dataset, valid_dataset, steps_per_epoch, validation_steps, valid_gen = dataLoader([\"1\",\"2\"], bs = 16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzCbvtvuxoWy"},"source":["#KF model\n","saveName = \"K_F\"\n","\n","## Architecture ##\n","tmodel = tf.keras.Sequential()\n","#Bottom\n","arch = tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3), pooling = \"avg\")\n","for layer in arch.layers[:650]:\n","  layer.trainable = False\n","tmodel.add(arch)\n","\n","#Top\n","tmodel.add(tf.keras.layers.Dropout(0.5))\n","tmodel.add(Dense(units=32,activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-2)))\n","\n","tmodel.add(tf.keras.layers.Dropout(0.5))\n","tmodel.add(Dense(units=32,activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-2)))\n","\n","# By multiple trials we decided to keep 2 dense layers with 32 units, indeed with a smaller number of classes, we don't need as many units\n","\n","#Output \n","tmodel.add(tf.keras.layers.Dense(units=2, activation='softmax'))\n","\n","## Optimization ##\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n","tmodel.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","tmodel.build(input_shape=(None, img_h, img_w, 3))\n","tmodel.summary()\n","\n","## Callbacks for ES and LR Plateau ##\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7,restore_best_weights=True)\n","LR_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.01, cooldown=0, min_lr=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8hcPbrhyV5C"},"source":["### Fitting ##\n","tmodel.fit(x=train_dataset,\n","           epochs=12,\n","           steps_per_epoch=steps_per_epoch, \n","           validation_data=valid_dataset,\n","           validation_steps=validation_steps,\n","           callbacks=[es_callback,LR_adapter_callback,]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-cdRM_WfzTi"},"source":["tmodel.save('Models/'+saveName)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wsV3Xsgd59f1"},"source":["# **5. Bayesian Ensemble**"]},{"cell_type":"code","metadata":{"id":"YZhxEIVY6Ir0"},"source":["from tensorflow.keras.models import load_model\n","\n","# Let us load our best models, they have different features: different output classes and input shapes\n","model_1=load_model('Models/K_3') # complete classifier\n","model02_1=load_model('Models/K_M') # distinguish between class 0 and 2\n","model12_1=load_model('Models/K_F') # distinguish between class 1 and 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOu_4XHN6N_D"},"source":["# auxiliary of the Bayesian classifier\n","def converter02(v):\n","  g=v[0]\n","  w=np.zeros(3)\n","  w[0]=g[0]\n","  w[1]=g[1]/2\n","  w[2]=g[1]/2\n","  return w\n","\n","# auxiliary of the Bayesian classifier\n","def converter12(v):\n","  g=v[0]\n","  w=np.zeros(3)\n","  w[0]=g[1]/2\n","  w[1]=g[0]\n","  w[2]=g[1]/2\n","  return w\n","\n","### this function takes as input three models, the first distinguishes between classes 0 and 2, the second between 1 and 2 and the third between all the three.\n","### in order to put toghether the predictions, we use this function which applies Bayes theorem\n","def bayesian_classifier(v02_1,v12_1,v_1): \n","  prior=np.array([1,1,1])   ### if the model has an high accuracy, it is better to use a flat prior that relies on the given results\n","\n","  a=converter02(v02_1)\n","  b=converter12(v12_1)\n","  c=v_1[0]\n","  \n","  return np.argmax(a*b*c*prior)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dFpZeFC6Qd0"},"source":["from PIL import Image\n","import os\n","from datetime import datetime\n","\n","\n","\n","image_filenames = next(os.walk('MaskDataset/test'))[2]\n","\n","results = {}\n","i=0\n","\n","img_h=612\n","img_w=408\n","\n","for image_name in image_filenames:\n","  \n","   img = Image.open('MaskDataset/test/'+image_name).convert('RGB')\n","\n","   img=img.resize((img_h,img_w))\n","   img_array = np.array(img)\n","   img_array = np.expand_dims(img_array, 0)\n","   img_array = np.true_divide(img_array,255)\n","\n","   v02_1=model02_1.predict(img_array)    # that's all for the 0 vs 2 classifier\n","   v12_1=model12_1.predict(img_array)    # that's all for the 1 vs 2 classifier\n","   v_1=model_1.predict(img_array)        # that's all for the three class model\n","\n","   results[image_name]=str(bayesian_classifier(v02_1,v12_1,v_1))  # the Bayesian classifier takes the three posterior probabilities\n","                                                                  # and computes the winner\n","\n","\n","### how to create the resulting csv file, think you know it better than us :)\n","def create_csv(results, results_dir='./'):\n","\n","    csv_fname = 'results_'\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n","\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n","\n","        f.write('Id,Category\\n')\n","\n","        for key, value in results.items():\n","            f.write(key + ',' + str(value) + '\\n')\n","\n","create_csv(results,'')"],"execution_count":null,"outputs":[]}]}